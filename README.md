## Recipe Aggregator

### Overview
Finding reliable recipes on the internet can be a challenge. Despite the abundance of information and options available, there is rarely a consensus on what ingredients belong in any given dish, and multiple highly-rated recipes for the same dish will sometimes differ dramatically. Which recipe is _actually_ the best? The most authentic? In all likelihood, the best version of the recipe is likely somewhere in the middle ground between a number of 'nearby' options.

With this in mind, my goal was to create a tool that would simplify and improve the online recipe search by condensing the overwhelming range of choices into a small handful of reliable and authentic recipes, each representing a common variation of the desired dish (e.g., search: 'chili' --> results: 'beef/tomato/bean chili', 'white chicken chili', etc.). Ultimately, I achieved this by collecting and vectorizing several thousand recipes from _allrecipes.com_, quantifying their mutual similarity using unsupervised learning algorithms--namely DBSCAN and graph-based clustering--and combining the results of this analysis with user ratings and reviews to generate 'optimized' versions of several different recipes.

### Data
#### Data collection
Though there are a variety of APIs available that provide access to recipe data for a reasonable price, I chose instead to scrape the data maunally from a single site, in part because it provided consistency in formatting between recipes (which proved invaluable), but more so because it represented a challenging yet valuable learning opportunity. Using a Selenium-based Python script, two EC2 instances were set up to crawl _allrecipes.com_ in parallel, scrape recipe data (i.e., name, cuisine (sub-)category, user reviews, and ingredients/proportions), and store it in a Mongo database. Approximately 2500 recipes were collected for this proof-of-concept project, most of them being slow-cooked dishes (e.g., soups, chilis, stews, curries, etc.), the rationale being that the quality of such dishes stems more from the ingredients and their proportions than it does the skill of the chef, thus making them ideal for a model that seeks to optimize the former, irrespective of the latter.

#### Ingredient parsing and normalization
Perhaps the largest challenge of this project was that, in order to compare recipes to one another on the basis of their ingredient proportions, it was necessary that all recipes' ingredient bills first be reduced to some standard form. While this is straightforward when directly comparing units of mass or volume, it is decidedly less so when comparing ingredients that are occasionally/often measured in counts (e.g., 1 medium onion vs. 1 cup onion vs. 1/2 pound onion). The issue is further complicated by the fact that ingredient lists commonly include unwanted descriptors (boneless, seedless) and preparation instructions (chopped, diced, grated) that make normalization of recipes all the more difficult. Nonetheless, using a complex parsing methodology that (i) identified quantitites/units, (ii) removed adjectives, adverbs and other unwanted words/phrases, (iii) merged similar ingredients, and (iv) converted all quantities to the same units, I was able to successfully convert the ingredient strings scraped from _allrecipes.com_ (e.g., `['2 (14.5 oz.) cans diced tomatoes', '1 large yellow onion, chopped']`) into a standard form consisting of only the ingredient and its approximate mass, in ounces (`{'tomato': 29, 'onion':8}`)

### Modeling
Once the recipes were 'vectorized' via the procedure described above, I was able to visualize, explore, and model the data using graph-based techniques and a modified version of the DBSCAN clustering algorithm.
#### Graph-based modeling
High-dimensional data, like that used in this project (`n_features` = # of unique ingredients), is notoriously difficult to visualize and interpret. Several common dimensionality reduction techniques, namely PCA and autocorrelation, failed to capture any meaningful structure to the recipe data; however, I found that the nuanced connections between similar recipes (and similar families of cuisine, more broadly) could be clearly conveyed using graphs. By representing each recipe as a node in the graph and then connecting recipe nodes with edges if the cosine similarity of their ingredient vectors is above some threshold (i.e., if thier ingredient _proportions_ are sufficiently similar, irrespective of the number of servings), it is possible to visualize the structure of the dataset with plots like that shown in Figure 1a.

Furthermore, the graph-based treatment of the data enables the use of the Girvan-Newman algorithm for clustering the data by iteratively finding and removing the edge with the highest _betweenness centrality_ until the graph splits into multiple disconnected components. When performed on a subset of the recipe dataset (recipes matching the search term 'chili'), this clustering algorithm naturally split the data into four distinctly different styles of chili (see Fig. 1b). Additionally, by using the eigenvector centrality of the recipe nodes as a proxy, I was able to estimate the relative authenticity of the recipes in each cluster and weight the model's outputs accordingly, as I will discuss below.

#### Modified DBSCAN
Although the graph-based models yielded excellent results, the Girvan-Newman clustering algorithm does not scale well to large or highly-connected datasets, due in large part to the _O(n^2)_ computational expense of calculating edge betweenness centrality. The DBSCAN algorithm is similar in many ways to the clustering methodology decribed above and can be used in this circumstance to achieve comparable results in a fraction of the time. As with the graph-based methods, cosine similarity was used as the distance metric for comparing recipes, and also as before, recipes with sufficiently high similarity were connected by edges. Nodes that were connected to at least `min_points` other nodes were considered core points and, along with all continuously adjacent core points, formed the backbone of each cluster. Unfortunately, because of the large variation in cluster sizes, this standard implementation of the DBSCAN algorithm failed to adequately capture the natural clustering of the recipe data (see Fig. 2), so a modified implementation was developed and employed. The key modification was the use of a recursive approach to break large clusters into smaller and smaller sub-clusters by gradually increasing the `min_points` hyperparameter. Ultimately, this approach was used to analyze the entire dataset of 2500 recipes, and the recipes within the resulting clusters showed remarkable similarity and specificity (e.g., Asian marinated/glazed salmon recipes), confirming the algorithm's success.


#### Recipe generation
