## Recipe Aggregator

### Overview
Finding reliable recipes on the internet can be a challenge. Despite the abundance of information and options available, there is rarely a consensus on what ingredients belong in any given dish, and multiple highly-rated recipes for the same dish will sometimes differ dramatically. Which recipe is _actually_ the best? The most authentic? In all likelihood, the best version of the recipe is likely somewhere in the middle ground between a number of similar options.

With this in mind, my goal was to create a tool that would simplify and improve the online recipe search by condensing the overwhelming range of choices into a small handful of reliable and authentic recipes, each representing a common variation of the desired dish (e.g., search: 'chili' --> results: 'beef/tomato/bean chili', 'white chicken chili', etc.). Ultimately, I achieved this by collecting and vectorizing several thousand recipes from _allrecipes.com_, quantifying their mutual similarity using unsupervised learning algorithms--namely DBSCAN and graph-based clustering--and combining the results of this analysis with user ratings and reviews to generate 'optimized' versions of several different recipes.

### Data
#### Data collection
Though there are a variety of APIs available that provide access to recipe data for a reasonable price, I chose instead to scrape the data maunally from a single site, in part because it provided consistency in formatting between recipes (which proved invaluable), but more so because it represented a challenging yet valuable learning opportunity. Using a Selenium-based Python script, two EC2 instances were set up to crawl _allrecipes.com_ in parallel, scrape recipe data (i.e., name, cuisine (sub-)category, user reviews, and ingredients/proportions), and store it in a Mongo database. Approximately 2500 recipes were collected for this proof-of-concept project, most of them being slow-cooked dishes (e.g., soups, chilis, stews, curries, etc.), the rationale being that the quality of such dishes stems more from the ingredients and their proportions than it does the skill of the chef, thus making them ideal for a model that seeks to optimize the former, irrespective of the latter.

#### Ingredient parsing and normalization
Perhaps the largest challenge of this project was that, in order to compare recipes to one another on the basis of their ingredient proportions, it was necessary that all recipes' ingredient bills first be reduced to some standard form. While this is straightforward when directly comparing units of mass or volume, it is decidedly less so when comparing ingredients that are often measured in counts (e.g., 1 medium onion vs. 1 cup onion vs. 1/2 pound onion). The issue is further complicated by the fact that ingredient lists commonly include unwanted descriptors (boneless, seedless) and preparation instructions (chopped, diced, grated) that make normalization of recipes all the more difficult. Nevertheless, using a complex parsing methodology that (i) identified quantitites/units, (ii) removed adjectives, adverbs and other unwanted words/phrases, (iii) merged similar ingredients, and (iv) converted all quantities to the same units, I was able to successfully convert the ingredient strings scraped from _allrecipes.com_ (e.g., `['2 (14.5 oz.) cans diced tomatoes', '1 large yellow onion, chopped']`) into a standard form consisting of only the ingredient and its approximate mass, in ounces (`{'tomato': 29, 'onion':8}`)

### Modeling
Once the recipes were 'vectorized' via the procedure described above, I was able to visualize, explore, and model the data using graph-based techniques and a modified version of the DBSCAN clustering algorithm.

#### Visualization
High-dimensional data, like that used in this project (`n_features` = # of unique ingredients), is notoriously difficult to visualize and interpret. Several common dimensionality reduction techniques, namely PCA and autocorrelation, failed to capture any meaningful structure to the recipe data; however, I found that the nuanced connections between similar recipes (and similar families of cuisine, more broadly) could be clearly conveyed using graph-based visualizations. By representing each recipe as a node in the graph and then connecting recipe nodes with edges if the cosine similarity of their ingredient vectors is above some threshold (i.e., if thier ingredient _proportions_ are sufficiently similar, irrespective of the number of servings), it is possible to visualize the structure of the dataset with plots like that shown in Figure 1a.

#### Modified DBSCAN
DBSCAN is a spatial clustering algorithm that lends itself particularly well to graph-like data, as it uses the edge count at each node to identify and assign cluster centers. As with the graph-based methods, cosine similarity was used as the distance metric for comparing recipes, and, also as before, recipes with sufficiently high similarity were connected by edges. Nodes that were connected to at least `min_points` other nodes were considered core points and, along with all continuously adjacent core points, formed the backbone of each cluster. However, because of the large variation in cluster sizes, this standard implementation of the DBSCAN algorithm failed to adequately capture the natural clustering of the recipe data (Fig. 1b), so a modified implementation was developed and employed. The key modification was the use of a recursive approach to break large clusters into smaller and smaller sub-clusters by gradually increasing the `min_points` hyperparameter. After using this modified alroithm to analyze the entire dataset of 2500 recipes (Fig. 1c), the recipes within the resulting clusters showed remarkable similarity and specificity (e.g., Asian marinated/glazed salmon recipes), confirming the algorithm's success.

#### Graph-based modeling
Though DBSCAN proved useful for identifying families of closely related recipes, it fell short of the project's overall goal——to aggregate similar recipes into a single optimized recipe——because it was unable to split clusters with sufficient granularity to isolate specific dishes. To overcome this shortcoming, I returned to a graph-based treatment of the DBSCAN clustered data and made use of the Girvan-Newman algorithm, which enabled more nuanced analysis of each cluster's structure. By iteratively finding and removing the edge with the highest _betweenness centrality_, this algorithm split the recipe graph into increasingly more disconnected components that, after a sufficient number of iterations, came to represent distinctly different versions of a similar dish. For example, when performed on a subset of the recipe dataset (recipes matching the search term 'chili'), this clustering algorithm naturally split the data into four distinctly different styles of chili (see Fig. 2).

In addition to the benefits of this clustering methodology, the use of graph-based techniques also enabled the calculation of centrality measures to evaluate the extent to which each recipe is related to the others in its cluster. In particular, by using the _eigenvector centrality_ of the recipe nodes as a proxy, I was able to estimate the relative authenticity of the recipes in each cluster and weight the model's outputs accordingly.

#### Recipe generation
